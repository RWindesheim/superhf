{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44ef2b9-624b-4932-8533-f8f38349c49e",
   "metadata": {},
   "source": [
    "# Test Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53e61b-886f-43bd-9072-e241d99aef21",
   "metadata": {},
   "source": [
    "### Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42de834b-aa80-4c19-a1f1-1a16a9a0b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fee6b7-7667-4eea-8978-b7137c21088f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7cbf85a",
   "metadata": {},
   "source": [
    "Following the guide from https://huggingface.co/docs/transformers/tasks/language_modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1400a9-a59d-476a-afda-c91e69b23fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superhf import skeleton\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6eb26b19",
   "metadata": {},
   "source": [
    "# Sample Usage of Open Assistant Reward model base (~700mb model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cdbd45a",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83176919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 988/988 [00:00<00:00, 267kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 738M/738M [00:47<00:00, 15.5MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 454/454 [00:00<00:00, 185kB/s]\n",
      "Downloading (…)\"spm.model\";: 100%|██████████| 2.46M/2.46M [00:00<00:00, 5.25MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 8.66M/8.66M [00:01<00:00, 7.85MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 23.0/23.0 [00:00<00:00, 8.64kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 173/173 [00:00<00:00, 82.3kB/s]\n"
     ]
    }
   ],
   "source": [
    "reward_name = \"OpenAssistant/reward-model-deberta-v3-base\"\n",
    "rank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name), AutoTokenizer.from_pretrained(reward_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d355309b",
   "metadata": {},
   "source": [
    "## Load and score a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d24479d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5816]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question, answer = \"Explain nuclear fusion like I am five\", \"Nuclear fusion is the process by which two or more protons and neutrons combine to form a single nucleus. It is a very important process in the universe, as it is the source of energy for stars and galaxies. Nuclear fusion is also a key process in the production of energy for nuclear power plants.\"\n",
    "inputs = tokenizer(question, answer, return_tensors='pt')\n",
    "score = rank_model(**inputs).logits.detach()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e0352b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8107]])\n"
     ]
    }
   ],
   "source": [
    "question, answer = \"Explain nuclear fusion like I am five\", \"Nuclear fusion is the process by which two or more protons and neutrons combine to form a single nucleus. Nuclear fusion is also a key process in the production of energy for nuclear power plants.\"\n",
    "inputs = tokenizer(question, answer, return_tensors='pt')\n",
    "score = rank_model(**inputs).logits.detach()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9be7675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2474]])\n"
     ]
    }
   ],
   "source": [
    "question, answer = \"Explain nuclear fusion like I am five\", \"Nuclear fusion is the process by which two or more protons and neutrons combine to form a single nucleus. Nuclear fusion is also a key process in the production of energy for nuclear power plants. Nuclear fusion is what powers the sun.\"\n",
    "inputs = tokenizer(question, answer, return_tensors='pt')\n",
    "score = rank_model(**inputs).logits.detach()\n",
    "print(score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4541a4e",
   "metadata": {},
   "source": [
    "# Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28be5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eli5 = load_dataset(\"eli5\", split=\"train_asks[:5000]\")\n",
    "# eli5 = eli5.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da24ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': 'itd2e',\n",
       " 'title': 'What could be accomplished if we were able to adjust the strength of each fundamental force?',\n",
       " 'selftext': 'If the force of gravity is only weak because we are separated from its concentration by one or more extra dimensions, then a similar separation from the areas of concentration of the other forces should also be possible, which would result in the adjustments mentioned in the title. So what would the ramifications of such adjustments be?',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers': {'a_id': ['c26hboo', 'c26jbsf'],\n",
       "  'text': [\"> If the force of gravity is only weak because we are separated from its concentration by one or more extra dimensions, then a similar separation from the areas of concentration of the other forces should also be possible,\\n\\nNo. This... really doesn't mean much of anything, I'm afraid. I can answer the last part of your question, but I have to point out that this whole premise part does not make sense.\\n\\nIf you could adjust the strengths of the fundamental forces you could do a lot of things. If you made electromagnetism too strong then atoms wouldn't be stable. If you made the strong force too weak, nuclei wouldn't be stable. There are lots of things that would be different if some combination of relative strengths were weaker or stronger, and I really don't know how to list out all of them.\",\n",
       "   'We could accidentally blow a hole in the universe if the fundamental forces were \"user serviceable\".'],\n",
       "  'score': [7, 2]},\n",
       " 'title_urls': {'url': []},\n",
       " 'selftext_urls': {'url': []},\n",
       " 'answers_urls': {'url': []}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "727696e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 762/762 [00:00<00:00, 322kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.12MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.25MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.43MB/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d454d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': 'itd2e',\n",
       " 'title': 'What could be accomplished if we were able to adjust the strength of each fundamental force?',\n",
       " 'selftext': 'If the force of gravity is only weak because we are separated from its concentration by one or more extra dimensions, then a similar separation from the areas of concentration of the other forces should also be possible, which would result in the adjustments mentioned in the title. So what would the ramifications of such adjustments be?',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers.a_id': ['c26hboo', 'c26jbsf'],\n",
       " 'answers.text': [\"> If the force of gravity is only weak because we are separated from its concentration by one or more extra dimensions, then a similar separation from the areas of concentration of the other forces should also be possible,\\n\\nNo. This... really doesn't mean much of anything, I'm afraid. I can answer the last part of your question, but I have to point out that this whole premise part does not make sense.\\n\\nIf you could adjust the strengths of the fundamental forces you could do a lot of things. If you made electromagnetism too strong then atoms wouldn't be stable. If you made the strong force too weak, nuclei wouldn't be stable. There are lots of things that would be different if some combination of relative strengths were weaker or stronger, and I really don't know how to list out all of them.\",\n",
       "  'We could accidentally blow a hole in the universe if the fundamental forces were \"user serviceable\".'],\n",
       " 'answers.score': [7, 2],\n",
       " 'title_urls.url': [],\n",
       " 'selftext_urls.url': [],\n",
       " 'answers_urls.url': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eli5 = eli5.flatten()\n",
    "# eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cf11a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d58e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0: 100%|██████████| 1/1 [00:01<00:00,  1.53s/ba]\n",
      "\n",
      "#2: 100%|██████████| 1/1 [00:01<00:00,  1.53s/ba]\n",
      "#0: 100%|██████████| 1/1 [00:01<00:00,  1.55s/ba]\n",
      "#3: 100%|██████████| 1/1 [00:01<00:00,  1.53s/ba]\n",
      "#1: 100%|██████████| 1/1 [00:01<00:00,  1.55s/ba]\n",
      "#0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "#3: 100%|██████████| 1/1 [00:00<00:00,  2.94ba/s]\n",
      "\n",
      "#1: 100%|██████████| 1/1 [00:00<00:00,  2.68ba/s]\n",
      "#2: 100%|██████████| 1/1 [00:00<00:00,  2.71ba/s]\n",
      "#0: 100%|██████████| 1/1 [00:00<00:00,  2.44ba/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenized_eli5 = eli5.map(\n",
    "#     preprocess_function,\n",
    "#     batched=True,\n",
    "#     num_proc=4,\n",
    "#     remove_columns=eli5[\"train\"].column_names,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "686da355",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb6a86c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#2: 100%|██████████| 1/1 [00:05<00:00,  5.11s/ba]\n",
      "#0: 100%|██████████| 1/1 [00:05<00:00,  5.19s/ba]\n",
      "\n",
      "\n",
      "#3: 100%|██████████| 1/1 [00:05<00:00,  5.25s/ba]\n",
      "#1: 100%|██████████| 1/1 [00:05<00:00,  5.44s/ba]\n",
      "#0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0: 100%|██████████| 1/1 [00:00<00:00,  3.47ba/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "#2: 100%|██████████| 1/1 [00:00<00:00,  3.50ba/s]\n",
      "#3: 100%|██████████| 1/1 [00:00<00:00,  3.63ba/s]\n",
      "#1: 100%|██████████| 1/1 [00:00<00:00,  3.30ba/s]\n"
     ]
    }
   ],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b336b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdf16676bbad4ddcd11952105f63537bbbd145d2903d460b604ae5df4d469172"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
