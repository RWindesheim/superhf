program: experiments/superhf/shf_iterative_v1/run_shf_iterative.py
method: grid
metric:
  goal: maximize
  name: average_score.max
parameters:
  # language_model_name:
  #   values:
  #     - "EleutherAI/pythia-1.4b-deduped"
  #     - "Rallio67/chip_1.4B_instruct_alpha"
  #     - "lambdalabs/pythia-1.4b-deduped-synthetic-instruct"
  #     - "theblackcat102/pythia-1b-deduped-sft"
      # - "EleutherAI/gpt-neo-1.3B"
      # - "EleutherAI/gpt-neo-2.7B"
      # - "gpt2-xl"
  # reward_model_name:
  #   values:
  #     - "OpenAssistant/reward-model-deberta-v3-large-v2"
  #     - "theblackcat102/reward-model-deberta-v3-base-v2"
  #     - "OpenAssistant/reward-model-electra-large-discriminator"
  # completion_filter_top_k:
  #   values:
  #     - 1
  #     - 2
  #     - 4
  #     - 8
  # superbatch_size:
  #   values:
  #     - 16
  #     - 32
  #     - 64
  #     - 96
  #     - 128
  prompt_dataset_names:
    values:
    - ["anthropic-red-team"]
    - ["anthropic-red-team", "anthropic-harmless-base"]
    - ["anthropic-red-team", "openai/webgpt_comparisons"]
    - ["anthropic-harmless-base", "openai/webgpt_comparisons"]
  inverse_loss_penalty:
    values:
      - 0.001
      - 0.005
  learning_rate:
    values:
      - 1.0e-5
      - 5.0e-6
      - 1.0e-6
