{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuperHF Best of N\n",
    "\n",
    "Initial implementation of the most basic form of Super HF:\n",
    "\n",
    "1. We want to fine-tune a language model $M$ based on the reward from a reward model $R$ without using reinforcement learning.\n",
    "1. We first get a list of prompts for our generations.\n",
    "1. Iteratively, in a loop, we:\n",
    "    1. Sample $p$ prompts from the training set without replacement.\n",
    "    1. Use $M$ to generate $n$ completions for each prompt ($p*n$ total).\n",
    "    1. Use $R$ to select the top 1 of the $n$ completions for each prompt ($p$ total).\n",
    "    1. Fine-tune $M$ on the $p$ best-of-$n$ completions.\n",
    "    1. Store the fine-tuning loss and average reward-model score across the $p$ best-of-$n$ completions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from superhf.data import get_superhf_prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_MODEL_NAME = \"eleutherai/gpt-neo-125M\"\n",
    "REWARD_MODEL_NAME = \"OpenAssistant/reward-model-deberta-v3-base\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "language_model = AutoModelForCausalLM.from_pretrained(LANGUAGE_MODEL_NAME).to(device)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(REWARD_MODEL_NAME).to(device)\n",
    "\n",
    "language_tokenizer = AutoTokenizer.from_pretrained(LANGUAGE_MODEL_NAME)\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "dataset = get_superhf_prompts(\"TODO\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
